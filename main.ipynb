{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NzLuYS6pwUv",
        "outputId": "8ae3523e-014a-413a-fb91-6c84bb610bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gCDMiNJVDAS"
      },
      "source": [
        "# **Diabetes Mellitus Prediction**\n",
        "\n",
        "\n",
        "> *   Step 1: Load the input Data\n",
        "> *   Step 2: Implement Gaussian Naive Bayesian classifier\n",
        "> *   Step 3: Build the classifier and check the correctness of Table building\n",
        "> *   Step 4 Improve the classifier for Ranking\n",
        "> *   Step 5: Make prediction and perform evaluation\n",
        "> *   Step 6: Generate results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGnqdEnXT20"
      },
      "source": [
        "## *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROPDPsjdXpSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr06keeMWKKu"
      },
      "source": [
        "you can take a look at the input feature & ground truth format:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5G-VlT4VkYY"
      },
      "source": [
        "Please split the dataset into training set and validation set\n",
        "\n",
        "> Note: The purpose of ***random_state*** is to ensure that you can reproduce the results each time you split your dataset. This is often helpful for debugging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_yxRh0tyyHR"
      },
      "source": [
        "# **1. Advanced Part**\n",
        "In advanced part, you need to implement the Gaussian Bayesian classifier:\n",
        "- input features: ***3 physiological features***\n",
        "- output prediction: ***diabetes_mellitus***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIXO1GcSAUtG"
      },
      "source": [
        "## Global attributes\n",
        "> You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrJu6pEAUQQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDZpQtgTzBAz"
      },
      "source": [
        "## Step 1: Load the input Data\n",
        "Load the input file **advanced_training.csv**\n",
        "> Note: please don't change the input var name ***training_df, testing_df, X, and y***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EICIDlzJy7NB"
      },
      "outputs": [],
      "source": [
        "# TODO: modify your file path\n",
        "training_df = pd.read_csv('advanced_training.csv')\n",
        "testing_df = pd.read_csv('advanced_testing.csv')\n",
        "y = training_df['diabetes_mellitus']\n",
        "X = training_df.drop('diabetes_mellitus', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTzwktNcQ0gV"
      },
      "source": [
        "you can check whether the standardization works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKKgJmzVzhy_"
      },
      "source": [
        "## Step 2: Implement Gaussian Naive Bayesian classifier\n",
        "In this part, you need to implement the Gaussian Naive Bayesian classifier.\n",
        "\n",
        "The main difference between Naive Bayesian and Gaussian Naive Bayesian is the likelihood part. For Gaussian NB, we can use the probability density function (PDF) of the ***Gaussian distribution*** (also known as the Normal distribution):\n",
        "\n",
        "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp({-\\frac{(x - \\mu)^2}{2\\sigma^2}})$$\n",
        "\n",
        "The reason we need to use Gaussian is that when the data type is continuous numbers instead of discrete numbers, we can't build a table by just counting all the possible cases. However, we can assume the data distribution follows a Gaussian (or Normal) distribution by calculating its mean and variance. Then, we can approximate the values, even if some records don't appear in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbKCkAvLzwHK"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayesian:\n",
        "    def build_table(self, X, y):\n",
        "        # classes for ground truth: there are only negative(0) and positive(1) for y (hospital_death)\n",
        "        self.classes = np.unique(y)\n",
        "        # record prior for two classes\n",
        "        self.class_priors = {}\n",
        "        # **feature_mean_var_table** is a 3D dictionary table:\n",
        "        # structure: [class]    [feature]           ['mean'] = mean\n",
        "        # structure: [class]    [feature]           ['var']  = var\n",
        "        # example:   [0]        ['gcs_eyes_apache'] ['mean'] = mean for feature='gcs_eyes_apache' when hospital_death=0\n",
        "        # example:   [0]        ['gcs_eyes_apache'] ['var']  = var for feature='gcs_eyes_apache' when hospital_death=0\n",
        "        self.feature_mean_var_table = {}\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_priors[c] = len(X_c) / len(X) # TODO: calculate prior\n",
        "            self.feature_mean_var_table[c] = {}\n",
        "            for feature in X.columns:\n",
        "                self.feature_mean_var_table[c][feature] = {}\n",
        "                # Calculate mean and var for each feature\n",
        "                self.feature_mean_var_table[c][feature]['mean'] = X_c[feature].mean() # TDOO: calculate the mean\n",
        "                # *** 10/19 note: make sure that if you call numpy.var, the ddof should set as 1 ***\n",
        "                self.feature_mean_var_table[c][feature]['var'] = X_c[feature].var(ddof=1)  # TODO: calculate the var\n",
        "\n",
        "    def _calculate_likelihood(self, x, mean, var):\n",
        "        likelihood = (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        return likelihood # TODO: calculate the Gaussian (normal) distribution pdf function as likelihoo\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X.to_dict(orient='records')]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        log_posteriors = []\n",
        "        # this for loop implement: log(posteior) = log(prior) + log(likelihood)\n",
        "        for c in self.classes:\n",
        "            log_prior = np.log(self.class_priors[c])\n",
        "            log_likelihood = 0\n",
        "            for feature, value in x.items():\n",
        "              #Here\n",
        "                mean = self.feature_mean_var_table[c][feature]['mean']\n",
        "                var = self.feature_mean_var_table[c][feature]['var']\n",
        "                log_likelihood += np.log(self._calculate_likelihood(value, mean, var)) # TODO: calculate the log likelihood\n",
        "            log_posterior = log_prior + log_likelihood\n",
        "            log_posteriors.append((c, log_posterior))\n",
        "        return max(log_posteriors, key=lambda x: x[1])[0] # TODO: Return the class with the highest logarithm of posterior probability as the predicted class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGcgNBK9gs2"
      },
      "source": [
        "## Step 3: Build the classifier and check the correctness of Table building\n",
        "You can easily build an instance of your classifier and then create the table.\n",
        "\n",
        "To check whether you have correctly built the table of the ***Gaussian Naive Bayesian classifier***, there is an example for you to ensure that your implementation is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52cFQayn8FGQ"
      },
      "outputs": [],
      "source": [
        "# Initialize and build_table the model\n",
        "gnb_classifier = GaussianNaiveBayesian()\n",
        "gnb_classifier.build_table(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC-bk7gT-2Ts"
      },
      "source": [
        "And you also need to output the dictionary variable ***feature_mean_var_table*** as a pickle file, which will be examined for correctness.\n",
        "> Note: Since this is for checking the implementation of the build_table method, please ensure that the input for your table building, ***X and y,*** is taken from the provided advanced_training.csv file ***BEFORE*** splitting the dataset into training and validation sets.\n",
        "\n",
        "> Hint: Two values for you to check the implementation correctness:\n",
        "\n",
        "\n",
        "> `gnb_classifier.feature_mean_var_table[0]['bmi']['mean']` is\n",
        "28.61544\n",
        "\n",
        "> `gnb_classifier.feature_mean_var_table[0]['bmi']['var']` is\n",
        "63.57263"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYzPyNIEU0l",
        "outputId": "fbcf210e-f62b-4eee-f090-645098c94e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pass\n"
          ]
        }
      ],
      "source": [
        "# *** 10/18 update: the value of mean and var***\n",
        "if round(gnb_classifier.feature_mean_var_table[0]['bmi']['mean'], 5) == 28.61544 and \\\n",
        "   round(gnb_classifier.feature_mean_var_table[0]['bmi']['var'], 5) == 63.57263:\n",
        "    print('pass')\n",
        "else:\n",
        "    print('fail')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDJLzdxqEUXN"
      },
      "outputs": [],
      "source": [
        "# TODO: change your path to save the pickle file\n",
        "pickle_file_path = 'advanced_table'\n",
        "with open(pickle_file_path, 'wb') as table_file:\n",
        "    pickle.dump(gnb_classifier.feature_mean_var_table, table_file)\n",
        "    table_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV_-5sKla5Gi"
      },
      "source": [
        "## Step 4 Improve the classifier for Ranking 15%:\n",
        "\n",
        "To make your model have better performance, you can try different ways to modify your model.\n",
        "\n",
        "> hints (**you don't need to follow the hints**):\n",
        "\n",
        "1. You can deal with the **outliers**\n",
        "2. You can try first **converting real numbers into discrete values** and then using Naive Bayesian for classification.\n",
        "3. You can try **def a new class for giving the prior a different weight** for decision-making.\n",
        "4. Anything you want to try based on Bayesian.\n",
        "\n",
        "> Note: You need to consider what kind of operations should also be performed on the testing_df."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlSzUz3hFRFF"
      },
      "source": [
        "### 4.1 Deal with outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t-kJoO9FYz4"
      },
      "outputs": [],
      "source": [
        "#Replace (ver1)\n",
        "def handle_outliers(df, features):\n",
        "  for feature in features:\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[feature] = df[feature].apply(lambda x: lower_bound if x < lower_bound else (upper_bound if x > upper_bound else x))\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "outlier_features = ['bmi', 'glucose_apache']\n",
        "training_df_no_outliers = handle_outliers(training_df, outlier_features)\n",
        "y = training_df_no_outliers['diabetes_mellitus']\n",
        "X = training_df_no_outliers.drop('diabetes_mellitus', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA76xJU1JP3f",
        "outputId": "ffb99cf4-a2e1-45b2-f4ac-bdc6588593fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65000"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHKzG1CJQYT",
        "outputId": "e8118464-cd42-466a-e037-76bd3b6dc5b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    1\n",
              "5    1\n",
              "6    1\n",
              "7    1\n",
              "8    0\n",
              "9    1\n",
              "Name: diabetes_mellitus, dtype: int64"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01E0R552_Yak"
      },
      "source": [
        "### 4.2 def a new class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toqd8h3E_bs6"
      },
      "outputs": [],
      "source": [
        "class CustomGaussianNaiveBayesian(GaussianNaiveBayesian):\n",
        "  def __init__(self, class_priors):\n",
        "    super().__init__()\n",
        "    self.custom_class_priors = class_priors\n",
        "\n",
        "  def build_table(self, X, y):\n",
        "    super().build_table(X, y)\n",
        "    self.class_priors = self.custom_class_priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRxEgVppItC9"
      },
      "source": [
        "### 4.3 Build Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "S_FhRPtTIxWf"
      },
      "outputs": [],
      "source": [
        "custom_priors = {0: 0.45, 1: 0.55}\n",
        "custom_classifier = CustomGaussianNaiveBayesian(custom_priors)\n",
        "custom_classifier.build_table(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3LMrpuGLUz"
      },
      "source": [
        "### 4.4 Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "mW781SgrH9Tj"
      },
      "outputs": [],
      "source": [
        "def adv_train_val_split(X, y, val_size, random_state):\n",
        "    # TODO: implement your own train_val_split\n",
        "    np.random.seed(random_state)\n",
        "    num_val_samples = int(val_size * len(X))\n",
        "    indices = np.arange(len(X))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    X_train = X.iloc[indices[num_val_samples:]]\n",
        "    X_val = X.iloc[indices[:num_val_samples]]\n",
        "    y_train = y.iloc[indices[num_val_samples:]]\n",
        "    y_val = y.iloc[indices[:num_val_samples]]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n",
        "# TODO: Split the data into training and validation sets\n",
        "# Note: please follow template for the format of return variables\n",
        "val_size = 0.2\n",
        "random_state = 42\n",
        "X_train_adv, X_val_adv, y_train_adv, y_val_adv = adv_train_val_split(X, y, val_size, random_state)# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtFVxggz7I4"
      },
      "source": [
        "## Step 5: Make predictions and perform evaluation\n",
        "You should test your model by evaluating the training set and validation set using the ***cal_f1_score*** function you implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "415sqX0D_jFY"
      },
      "outputs": [],
      "source": [
        "... # TODO: build table on the training set\n",
        "... # TODO: Make predictions on training set and calculate the f1-score\n",
        "... # TODO: Make predictions on validation set and calculate the f1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "SUu0bDlTKP82"
      },
      "outputs": [],
      "source": [
        "def cal_f1_score(y_true, y_pred):\n",
        "    # Calculate True Positives, False Positives, False Negatives\n",
        "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
        "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1) # TODO: calculate the false positive\n",
        "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0) # TODO: calculate the false negative\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0  # TDOO: calculate the precision\n",
        "    recall =  tp / (tp + fn) if (tp + fn) > 0 else 0 # TODO: calculate the recall\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 # TODO: calculate the f1-score\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL9DK49_J-rI",
        "outputId": "ecf96897-0cec-4d7e-bf3f-75a0b7c37290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training F1-score adv: 0.5511537865881989\n",
            "Validation F1-score adv: 0.5553304634084578\n"
          ]
        }
      ],
      "source": [
        "# TODO: build table on the training set\n",
        "# TODO: Make predictions on training set and calculate the f1-score\n",
        "train_predictions_adv = custom_classifier.predict(X_train_adv)\n",
        "train_f1_score_adv = cal_f1_score(y_train_adv, train_predictions_adv)\n",
        "print(\"Training F1-score adv:\", train_f1_score_adv)\n",
        "\n",
        "# TODO: Make predictions on validation set and calculate the f1-score\n",
        "val_predictions_adv = custom_classifier.predict(X_val_adv)\n",
        "val_f1_score_adv = cal_f1_score(y_val_adv, val_predictions_adv)\n",
        "print(\"Validation F1-score adv:\", val_f1_score_adv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJWF-tfA_Q9d"
      },
      "source": [
        "## Step 6: Generate result\n",
        "> Note: Please follow the format mentioned in the slides. You can only change the path for saving your code down below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "ZX6FV9tRK6Fp"
      },
      "outputs": [],
      "source": [
        "predictions = custom_classifier.predict(testing_df) # TODO: predict on the testing_df\n",
        "\n",
        "# TODO: Specify the CSV file path\n",
        "csv_file_path = 'advanced_prediction.csv'\n",
        "\n",
        "# Write the predictions to the CSV file\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # *** 10/21 update: header name ***\n",
        "    writer.writerow(['diabetes_mellitus'])\n",
        "    for prediction in predictions:\n",
        "        writer.writerow([prediction])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
